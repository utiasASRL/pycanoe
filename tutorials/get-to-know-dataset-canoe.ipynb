{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed3e340-17fd-4b71-a98e-c776aa45d053",
   "metadata": {},
   "source": [
    "# Get to Know a Dataset: Canoe\n",
    "\n",
    "This notebook serves as a guided tour of the [CANOE](https://registry.opendata.aws/canoe) dataset. More usage examples, tutorials, and documentation for this dataset and others can be found at the [Registry of Open Data on AWS](https://registry.opendata.aws/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec723da5",
   "metadata": {},
   "source": [
    "### *Questions to answer*: 1) How have you organized your dataset? Help us understand the key prefix structure of your S3 bucket. 2) What data formats are present in your dataset? What kinds of data are stored using these formats? Can you give any advice for how you work with these data formats? 3) A picture is worth a thousand words. Show us a visual (or several!) from your dataset that either illustrates something informative about your dataset, or that you think might excite someone to dig in further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3779654-eeee-4708-83cf-245e03303475",
   "metadata": {},
   "source": [
    "#### Data Organization\n",
    "Our dataset is stored under a single AWS S3 bucket. \n",
    "At the top level of our bucket is the `data/` prefix and, in the future, some high-level documentation.\n",
    "For now, full documentation can be found at [https://github.com/utiasASRL/pycanoe/blob/main/DATA_REFERENCE.md](https://github.com/utiasASRL/pycanoe/blob/main/DATA_REFERENCE.md).\n",
    "Under the `data/` prefix, our dataset is organized as a set of sequences. \n",
    "Each sequence is stored as a folder and follows the same naming convention: `s3://canoe-data/data/canoe-YYYY-MM-DD-HH-MM` denoting the time that data collection started. \n",
    "Below is an overview of the structure of each sequence:\n",
    "\n",
    "```text\n",
    "canoe-YYYY-MM-DD-HH-MM/\n",
    "    calib/\n",
    "        P_cam.txt\n",
    "        radar_config.yaml\n",
    "        T_sens2_sens1.txt\n",
    "    novatel/\n",
    "        novatel_imu.csv\n",
    "        novatel_original.csv\n",
    "        <sens>_poses.csv\n",
    "    cam_left/\n",
    "        <timestamp>.png\n",
    "    cam_right/\n",
    "        <timestamp>.png\n",
    "    imu/\n",
    "        imu.csv\n",
    "    lidar/\n",
    "        <timestamp>.bin\n",
    "    motor/\n",
    "        power.csv\n",
    "    radar/\n",
    "        <timestamp>.png\n",
    "    sonar/\n",
    "        <timestamp>.png\n",
    "    cam.mp4\n",
    "    dashboard.mp4\n",
    "    route_map.html\n",
    "```\n",
    "\n",
    "Sequence files fall under a few categories:\n",
    "- **Overview Files:** At the top level of the sequence folder, there are three files that offer a high-level look at the contents of the data collection run.\n",
    "The videos show a 10$\\times$ speed feed of the camera data (`cam.mp4`) and \"dashboard\" data (camera, radar, lidar, sonar, and motor inputs) (`dashboard.mp4`) for the whole sequence with the time in the lower left corner.\n",
    "The `route_map.html` opens an interactive satellite map with the vehicle route overlaid on top.\n",
    "- **Calibration Files:** Calibration files include `.txt` and `.yaml` files that contain either sensor properties/configurations or the extrinsic calibration between two sensors.\n",
    "- **Sensor Files:** The `cam_left/`, `cam_right/`, `lidar/`, `radar/`, and `sonar/` folders have a single file for each sensor measurement (named with the timestamp). Besides lidar, which uses a `.bin` format to reduce storage requirements, the sensor readings are stored in `.png` format.\n",
    "- **Auxillary Sensor Files:** The IMU and motor input measurements each use one `.csv` file for the whole dataset.\n",
    "- **GPS Files**: In the novatel folder are `.csv` files that contain the post-processed GNSS measurements. For each sensor, there is a file `<sensor>_poses.csv` that contains the GNSS-measured position and velocity interpolated to the sensor timestamp and transformed to the sensor frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9614d9",
   "metadata": {},
   "source": [
    "#### Accessing the data\n",
    "Our dataset is intended to be downloaded and used locally.\n",
    "\n",
    "The main S3 bucket can be browsed through using the S3 console in your internet browser at: \n",
    "[`https://s3.console.aws.amazon.com/s3/buckets/canoe-data/`](https://s3.console.aws.amazon.com/s3/buckets/canoe-data/)\n",
    "\n",
    "We recommend accessing and downloading the dataset through the AWS CLI as follows\n",
    "1. [Create an AWS account (optional)](https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/)\n",
    "2. [Install the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)\n",
    "3. Create a `root` folder to store the dataset, example: `/path/to/data/canoe/` Each sequence will then be a folder under `root`.\n",
    "4. Use the AWS CLI to download either the entire dataset or only the desired sequences and sensors. Add `--no-sign-request` after each of the following commands if you're not going to use an AWS account. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a21d87a",
   "metadata": {},
   "source": [
    "##### Explore Data\n",
    "The `aws s3 ls` command can be used to look through the folders. For example, the following command will list all the sequences:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6349122",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!aws s3 ls s3://canoe-data/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e738f",
   "metadata": {},
   "source": [
    "##### Download Sample Sequence\n",
    "The `aws s3 sync` command is used to download data. For example, the following command downloads the sequence `canoe-2025-08-21-19-16`.\n",
    "This sequence is our **sample sequence**. It contains approximately 90 seconds of data and contains ~8GB of data.\n",
    "We recommend downloading this sequence if you want to explore and try loading the data for yourself. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c243168e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "%%bash\n",
    "root=/path/to/data/canoe/\n",
    "cd $root\n",
    "aws s3 sync s3://canoe-data/data/canoe-2025-08-21-19-16 ./canoe-2025-08-21-19-16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b6f58",
   "metadata": {},
   "source": [
    "#### Loading Data with the pycanoe DevKit\n",
    "Now that you have a sequence downloaded, you can load it with the SDK, `pycanoe`.\n",
    "\n",
    "##### Installations\n",
    "First, if you haven't already done so, clone the pycanoe repo with `git clone git@github.com:utiasASRL/pycanoe.git` and install it locally with `pip install -e pycanoe`.\n",
    "\n",
    "This will install the requirements for `pycanoe`, which includes the following libraries (see pycanoe/`setup.py` for full updated list):\n",
    "- numpy\n",
    "- matplotlib\n",
    "- opencv-python>=4.5.3.56\n",
    "- PyYAML>=5.4.0\n",
    "- scipy>=1.14.0\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bdace470",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "! pip install /path/to/pycanoe/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b78a41",
   "metadata": {},
   "source": [
    "##### Import SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycanoe\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717987e-c9c9-4c0f-9f2e-a8d761492625",
   "metadata": {},
   "source": [
    "##### Load Dataset with pycanoe\n",
    "Below loads the entire dataset and shows some iterating/accessing operations.\n",
    "It is also possible to load individual sequences or sensor objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycanoe import CanoeDataset\n",
    "\n",
    "root = '/path/to/data/canoe/'\n",
    "cd = CanoeDataset(root, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The CANOE dataset differs from others (e.g., KITTI) in that \n",
    "# measurements, from different sensors are not synchronous. However, \n",
    "# each sensor message has an accurate timestamp and pose instead.\n",
    "\n",
    "# Loop through each frame in order (e.g.,  for odometry)\n",
    "for seq in cd.sequences:\n",
    "    # Iterator examples:\n",
    "    for camera_frame in seq.camleft:\n",
    "        img = camera_frame.img  \n",
    "        # do something\n",
    "        camera_frame.unload_data() # Memory reqs will keep increasing without this\n",
    "    for lidar_frame in seq.lidar:\n",
    "        pts = lidar_frame.points  \n",
    "        # do something\n",
    "        lidar_frame.unload_data() # Memory reqs will keep increasing without this\n",
    "\n",
    "    # Retrieve frames based on their index:\n",
    "    N = len(seq.radar_frames)\n",
    "    for i in range(N):\n",
    "        radar_frame = seq.get_radar(i)\n",
    "        # do something\n",
    "        radar_frame.unload_data() # Memory reqs will keep increasing without this\n",
    "\n",
    "# Iterator example:\n",
    "cam_iter = cd.sequences[0].get_cam_left_iter()\n",
    "cam0 = next(cam_iter)  # First camera frame\n",
    "cam1 = next(cam_iter)  # Second camera frame\n",
    "\n",
    "# Randomly access frames (e.g., for deep learning, localization):\n",
    "N = len(cd.lidar_frames)\n",
    "indices = np.random.permutation(N)\n",
    "for idx in indices:\n",
    "    lidar_frame = cd.get_lidar(idx)\n",
    "    # do something\n",
    "    lidar_frame.unload_data() # Memory reqs will keep increasing without this\n",
    "\n",
    "# Each sequence contains a calibration object:\n",
    "calib = cd.sequences[0].calib\n",
    "point_lidar = np.array([1, 0, 0, 1]).reshape(4, 1)\n",
    "point_camera = np.matmul(calib.T_camleft_lidar, point_lidar)\n",
    "\n",
    "# Each sensor frame has a timestamp, groundtruth pose\n",
    "# (4x4 homogeneous transform) wrt a fixed global coordinate frame (ENU_ref),\n",
    "# and groundtruth velocity information. Unless it's a part of the test set,\n",
    "# in that case, ground truth poses will be missing. \n",
    "lidar_frame = cd.get_lidar(0)\n",
    "t = lidar_frame.timestamp  # timestamp in seconds\n",
    "T_enu_lidar = lidar_frame.pose  # 4x4 homogenous transform [R t; 0 0 0 1]\n",
    "vbar = lidar_frame.velocity  # 6x1 vel in ENU frame [v_se_in_e; w_se_in_e]\n",
    "varpi = lidar_frame.body_rate  # 6x1 vel in sensor frame [v_se_in_s; w_se_in_s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130e273",
   "metadata": {},
   "source": [
    "##### Visualize Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First \"synchronize\". \n",
    "# NOTE: This doesn't actually synchronize frames, just grabs the closest \n",
    "seq = cd.sequences[0]\n",
    "seq.synchronize_frames(ref=\"cam_left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize frames at same index\n",
    "idx = 0\n",
    "\n",
    "# Camera\n",
    "cam = seq.camleft_frames[idx]\n",
    "cam.load_data()\n",
    "cam.visualize()\n",
    "cam.unload_data()\n",
    "\n",
    "# Radar\n",
    "rad = seq.radar_frames[idx]\n",
    "rad.load_data()\n",
    "rad.visualize()\n",
    "rad.unload_data()\n",
    "\n",
    "# Lidar\n",
    "lid = seq.lidar_frames[idx]\n",
    "lid.load_data()\n",
    "lid.visualize()\n",
    "lid.unload_data()\n",
    "\n",
    "# Sonar\n",
    "sonar = seq.sonar_frames[idx]\n",
    "sonar.load_data()\n",
    "sonar.visualize()\n",
    "sonar.unload_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c8b85-ed1c-4f2c-bd0e-fbfbc67c4723",
   "metadata": {},
   "source": [
    "### Q: What is one question that you have answered using these data? Can you show us how you came to that answer?\n",
    "One question that we have partially answered using this data is:\n",
    "**how large of a barrier are weeds to autonomous aquatic navigation?**\n",
    "\n",
    "Long weeds can get caught in the rotors of an unmanned surface vessel as shown in the picture below.\n",
    "\n",
    "![Weeds in Rotor](../figs/weeds.png \"Weeds in Rotor\")\n",
    "\n",
    "This slows the boat down and requires more power to maintain the same speed, which can drain the battery\n",
    "Without preventative measures, there will be upper limits on the length of time that an unmanned\n",
    "surface vessel can operate autonomously before human intervention is needed. \n",
    "\n",
    "This issue is visible in the dashboard videos. \n",
    "\n",
    "Below are two screenshots of the dashboard video from one of the reservoir sequences. \n",
    "On the second lap around the pillars, the boat requires much more power to achieve the same speed.\n",
    "\n",
    "| Lap 1 | Lap 2| \n",
    "| --- | --- |\n",
    "| ![Power Lap 1](../figs/power-1.png \"Power Lap 1\") | ![Power Lap 2](../figs/power-2.png \"Power Lap 2\") |\n",
    "\n",
    "\n",
    "This poses an interesting questino about how perception algorithms could be used to detect and avoid weeds. \n",
    "Preventative monitoring could be done with the camera, \n",
    "but there is also opportunity to explore the use of sonar data for this purpose, as thick patches of weeds \n",
    "are occassionally visible to the human eye in the sonar feed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf645724-3108-4ada-a832-10b3431eb8e2",
   "metadata": {},
   "source": [
    "### Q: What is one unanswered question that you think could be answered using these data? Do you have any recommendations or advice for someone wanting to answer this question?\n",
    "\n",
    "One question that we would like to answer using this data is:\n",
    "**How can we best leverage the multi-sensor suite to perform odometry or localization on a semi-open body of water?\n",
    "\n",
    "Each sensor has its unique strengths that can be leveraged for navigation under different conditions. \n",
    "Camera and lidar sensors provide rich information, but on a wide body of water such as a lake, the shoreline can be too far away to maintain a reliable signal.\n",
    "The radar sensor, on the other hand, has a much longer range and could prove useful for navigating with respect to the shoreline.\n",
    "Sonar data could be used to track one's position relative to the lake floor. \n",
    "\n",
    "Benchmarking state-of-the-art odometry and localization algorithms\n",
    "would offer further insight into the gaps and key challenges facing autonomous aquatic navigation, \n",
    "and the unique combination of \n",
    "sensors in this dataset affords more flexibility and creativity in \n",
    "future improvements of these algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
